[model]
thread_num = 4
pretrain_iter = 1
train_iter = 100
word_num = 10
letter_num = 10
observation_dim = 3

[pyhlm]
num_states = ${model:word_num}
alpha = 10
gamma = 10
init_state_concentration = 10

[letter_observation]
mu_0 = numpy.zeros(${model:observation_dim})
sigma_0 = numpy.identity(${model:observation_dim})
kappa_0 = 0.01
nu_0 = ${model:observation_dim} + 5

[letter_duration]
alpha_0 = 200
beta_0 = 10

[letter_hsmm]
alpha = 10
gamma = 10
init_state_concentration = 10

[superstate]
trunc = None

[word_length]
alpha_0 = 30
beta_0 = 10
